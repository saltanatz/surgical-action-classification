# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15xh-CE0ZcqpQ_WVEMQLamSASTEZOPLa1
"""

"""
Load Existing TSM Checkpoint + Add Flow Stream (On-the-Fly)
Fastest solution when flows aren't precomputed
"""

import os
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
import torchvision.models as models
from torchvision.models import ResNet18_Weights
import pandas as pd
import cv2
import numpy as np
from pathlib import Path
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report
import seaborn as sns
import time
import json


# ==================== CONFIGURATION ====================
CONFIG = {
    'base_path': Path('/content/LaparoMotion_fast/laparomotion_consistent/extracted'),
    'num_classes': 7,
    'batch_size': 4,
    'num_epochs': 15,               # Less epochs since adding flow to existing model
    'learning_rate': 1e-3,          # Lower LR for fine-tuning
    'weight_decay': 0,
    'num_workers': 2,
    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),
    'seed': 42,
    'checkpoint_dir': Path(''),
    'checkpoint_load': Path('/home/zhadiger/Desktop/data_preprocessing/best_model_twostream.pth'),  # YOUR EXISTING CHECKPOINT
    'use_flow': True,
    'tau_sampling': 0.5,
    'model_depth': 18,
}


CONFIG['checkpoint_dir'].mkdir(exist_ok=True)
torch.manual_seed(CONFIG['seed'])
np.random.seed(CONFIG['seed'])


print(f"Device: {CONFIG['device']}")
print(f"Loading checkpoint from: {CONFIG['checkpoint_load']}")
print(f"Checkpoint Directory: {CONFIG['checkpoint_dir']}")

# ==================== UTILITY FUNCTIONS ================

def create_class_balanced_sampler(csv_path, label_col='label', tau=0.5):
    """Create tempered class-balanced sampler."""
    df = pd.read_csv(csv_path)
    labs = df[label_col].astype(str)
    counts = labs.value_counts()

    w_per_class = {lab: (counts[lab] ** (-tau)) for lab in counts.index}
    m = sum(w_per_class.values()) / len(w_per_class)
    w_per_class = {k: v / m for k, v in w_per_class.items()}

    sample_weights = labs.map(lambda x: w_per_class[x]).astype(float).values

    sampler = WeightedRandomSampler(
        weights=torch.as_tensor(sample_weights, dtype=torch.double),
        num_samples=len(sample_weights),
        replacement=True
    )

    print(f"Class weights (tau={tau}):")
    for lab, w in sorted(w_per_class.items()):
        print(f"  {lab}: {w:.4f}")

    return sampler

# ==================== TSM IMPLEMENTATION ====================

class TemporalShift(nn.Module):
    """Temporal Shift Module"""
    def __init__(self, num_segments: int, fold_div: int = 8) -> None:
        super().__init__()
        self.num_segments = num_segments
        self.fold_div = fold_div

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        b, t, c, h, w = x.shape
        fold = c // self.fold_div

        if fold == 0:
            return x

        x1 = x[:, :, :fold, :, :]
        x2 = x[:, :, fold:2 * fold, :, :]
        x3 = x[:, :, 2 * fold:, :, :]

        x1_shifted = torch.zeros_like(x1)
        x1_shifted[:, 1:, ...] = x1[:, :-1, ...]

        x2_shifted = torch.zeros_like(x2)
        x2_shifted[:, :-1, ...] = x2[:, 1:, ...]

        x_shifted = torch.cat([x1_shifted, x2_shifted, x3], dim=2)
        return x_shifted


class StageWrapper(nn.Module):
    """Wraps ResNet stage with TSM"""
    def __init__(self, stage: nn.Module, num_segments: int, fold_div: int = 8):
        super().__init__()
        self.stage = stage
        self.tsm = TemporalShift(num_segments, fold_div)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.tsm(x)
        b, t, c, h, w = x.shape
        x = x.view(b * t, c, h, w)
        x = self.stage(x)
        if x.dim() == 4:
            _, c, h, w = x.shape
            x = x.view(b, t, c, h, w)
        return x


class ResNetTSM_OF(nn.Module):
    """ResNet with TSM - Two-Stream (Flow computed on-the-fly)"""

    def __init__(self, num_classes: int, num_segments: int = 16,
                 depth: int = 18, pretrained: bool = True, use_flow: bool = True):
        super().__init__()
        self.num_classes = num_classes
        self.num_segments = num_segments
        self.use_flow = use_flow

        # Build backbone
        if depth == 18:
            resnet = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1 if pretrained else None)
        else:
            raise ValueError(f"Only ResNet18 supported")

        # RGB stream components
        self.conv1 = resnet.conv1
        self.bn1 = resnet.bn1
        self.relu = resnet.relu
        self.maxpool = resnet.maxpool

        self.layer1 = StageWrapper(resnet.layer1, num_segments)
        self.layer2 = StageWrapper(resnet.layer2, num_segments)
        self.layer3 = StageWrapper(resnet.layer3, num_segments)
        self.layer4 = StageWrapper(resnet.layer4, num_segments)

        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))

        feat_dim = 512

        # RGB stream head
        self.rgb_fc = nn.Linear(feat_dim, feat_dim // 2)

        # Flow stream (if enabled)
        if use_flow:
            self.flow_conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
            self.flow_bn1 = nn.BatchNorm2d(64)
            self.flow_layer1 = StageWrapper(resnet.layer1, num_segments)
            self.flow_layer2 = StageWrapper(resnet.layer2, num_segments)
            self.flow_layer3 = StageWrapper(resnet.layer3, num_segments)
            self.flow_layer4 = StageWrapper(resnet.layer4, num_segments)

            self.flow_fc = nn.Linear(feat_dim, feat_dim // 2)
            self.fusion = nn.Linear(feat_dim, feat_dim // 2)

        # Simple classifier
        self.classifier = nn.Linear(feat_dim // 2, num_classes)

    def forward_rgb(self, x):
        """Process RGB stream"""
        b, t, c, h, w = x.shape

        x = x.view(b * t, c, h, w)
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)
        x = x.view(b, t, -1, h // 4, w // 4)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        b, t, c, h, w = x.shape
        x = x.view(b * t, c, h, w)
        x = self.avgpool(x)
        x = x.view(b, t, -1)
        x = x.mean(dim=1)

        x = self.rgb_fc(x)
        return x

    def forward_flow(self, x):
        """Process Flow stream"""
        b, t, c, h, w = x.shape

        x = x.view(b * t, c, h, w)
        x = self.flow_conv1(x)
        x = self.flow_bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)
        x = x.view(b, t, -1, h // 4, w // 4)

        x = self.flow_layer1(x)
        x = self.flow_layer2(x)
        x = self.flow_layer3(x)
        x = self.flow_layer4(x)

        b, t, c, h, w = x.shape
        x = x.view(b * t, c, h, w)
        x = self.avgpool(x)
        x = x.view(b, t, -1)
        x = x.mean(dim=1)

        x = self.flow_fc(x)
        return x

    def forward(self, rgb, flow=None):
        """Forward pass - two-stream"""
        rgb_feat = self.forward_rgb(rgb)

        if self.use_flow and flow is not None:
            flow_feat = self.forward_flow(flow)
            feat = torch.cat([rgb_feat, flow_feat], dim=1)
            feat = self.fusion(feat)
        else:
            feat = rgb_feat

        logits = self.classifier(feat)
        return logits


# ==================== DATASET ====================

class SurgicalVideoDataset(Dataset):
    """Dataset loader - compute optical flow on-the-fly"""

    def __init__(self, csv_path, base_path, target_frames=16, return_5d=True):
        self.base_path = Path(base_path)
        self.target_frames = target_frames
        self.return_5d = return_5d
        self.data = pd.read_csv(csv_path)

        self.path_col = 'file_path' if 'file_path' in self.data.columns else 'video_path'
        self.label_col = 'label' if 'label' in self.data.columns else 'action_label'

        unique_labels = sorted(self.data[self.label_col].unique())
        self.label2idx = {label: idx for idx, label in enumerate(unique_labels)}
        self.idx2label = {idx: label for label, idx in self.label2idx.items()}

        print(f"Dataset: {len(self.data)} samples, {len(self.label2idx)} classes")
        print(f"Classes: {self.idx2label}")

    def load_video(self, video_path):
        full_path = self.base_path / video_path
        if not full_path.exists():
            raise FileNotFoundError(f"Video not found: {full_path}")

        cap = cv2.VideoCapture(str(full_path))
        frames = []

        while len(frames) < self.target_frames:
            ret, frame = cap.read()
            if not ret:
                break
            frame = cv2.resize(frame, (224, 224))
            frames.append(frame)

        cap.release()

        if len(frames) == 0:
            raise RuntimeError(f"Could not read frames from {full_path}")

        while len(frames) < self.target_frames:
            frames.append(frames[-1])

        return frames[:self.target_frames]

    def preprocess_frame(self, frame):
        frame = frame.astype(np.float32) / 255.0
        frame = torch.tensor(frame).permute(2, 0, 1)
        return frame

    def compute_optical_flow(self, frames):
        """Compute optical flow on-the-fly"""
        flow_list = []
        for i in range(len(frames) - 1):
            prev_gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)
            next_gray = cv2.cvtColor(frames[i + 1], cv2.COLOR_BGR2GRAY)
            flow = cv2.calcOpticalFlowFarneback(prev_gray, next_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)
            flow_list.append(flow)

        # Pad with last flow
        if flow_list:
            flow_list.append(flow_list[-1])
        else:
            flow_list = [np.zeros((224, 224, 2), dtype=np.float32)]

        flow_array = np.array(flow_list, dtype=np.float32)

        # Create 3-channel flow representation
        flow_u = flow_array[..., 0]
        flow_v = flow_array[..., 1]
        flow_mag = np.sqrt(flow_u**2 + flow_v**2 + 1e-5)

        flow_u_norm = (flow_u - flow_u.mean()) / (flow_u.std() + 1e-5)
        flow_v_norm = (flow_v - flow_v.mean()) / (flow_v.std() + 1e-5)
        flow_mag_norm = (flow_mag - flow_mag.mean()) / (flow_mag.std() + 1e-5)

        flow_img = np.stack([flow_u_norm, flow_v_norm, flow_mag_norm], axis=-1)
        flow_frames = [self.preprocess_frame(f) for f in flow_img]
        return torch.stack(flow_frames)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        video_path = row[self.path_col]
        label = row[self.label_col]
        label_idx = self.label2idx[label]

        frames = self.load_video(video_path)
        rgb_frames = torch.stack([self.preprocess_frame(f) for f in frames])

        # Compute flow on-the-fly
        flow_frames = self.compute_optical_flow(frames)

        if self.return_5d:
            rgb_frames = rgb_frames.unsqueeze(0)
            flow_frames = flow_frames.unsqueeze(0)

        return rgb_frames, flow_frames, label_idx


# ==================== TRAINING ====================

def train_epoch(model, train_loader, optimizer, criterion, device, scaler):
    """Fast training"""
    model.train()
    total_loss = 0.0
    all_preds, all_labels = [], []

    start_time = time.time()
    pbar = tqdm(train_loader, desc="Training")

    for rgb, flow, labels in pbar:
        if rgb.dim() == 6:
            rgb = rgb.squeeze(1)
            flow = flow.squeeze(1)

        rgb = rgb.to(device)
        flow = flow.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()
        with torch.amp.autocast(device_type='cuda'):
            outputs = model(rgb, flow)
            loss = criterion(outputs, labels)

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        total_loss += loss.item()
        preds = torch.argmax(outputs, dim=1).cpu().numpy()
        all_preds.extend(preds)
        all_labels.extend(labels.cpu().numpy())

        pbar.set_postfix({'loss': loss.item()})

    epoch_time = time.time() - start_time
    acc = accuracy_score(all_labels, all_preds)

    return total_loss / len(train_loader), acc, epoch_time


def validate(model, val_loader, criterion, device):
    """Fast validation"""
    model.eval()
    total_loss = 0.0
    all_preds, all_labels = [], []
    total_inference_time = 0.0
    total_samples = 0

    with torch.no_grad():
        for rgb, flow, labels in tqdm(val_loader, desc="Validation"):
            if rgb.dim() == 6:
                rgb = rgb.squeeze(1)
                flow = flow.squeeze(1)

            rgb = rgb.to(device)
            flow = flow.to(device)
            labels = labels.to(device)

            start_time = time.time()
            outputs = model(rgb, flow)
            inference_time = time.time() - start_time

            total_inference_time += inference_time
            total_samples += rgb.size(0)

            loss = criterion(outputs, labels)
            total_loss += loss.item()
            preds = torch.argmax(outputs, dim=1).cpu().numpy()
            all_preds.extend(preds)
            all_labels.extend(labels.cpu().numpy())

    avg_inference_time = total_inference_time / total_samples
    acc = accuracy_score(all_labels, all_preds)

    return total_loss / len(val_loader), acc, all_preds, all_labels, avg_inference_time


# ==================== MAIN ====================

def main(csv_files):
    """Main training function"""

    required_keys = {'train.csv', 'val.csv', 'test.csv'}
    if not isinstance(csv_files, dict) or not required_keys.issubset(csv_files.keys()):
        print(f"Error: csv_files must be a dict with keys {required_keys}")
        return

    # Create class-balanced sampler
    train_sampler = create_class_balanced_sampler(
        csv_files['train.csv'],
        label_col='label',
        tau=CONFIG['tau_sampling']
    )

    # Load datasets
    print("\nLoading datasets...")
    train_dataset = SurgicalVideoDataset(
        csv_files['train.csv'], CONFIG['base_path'], return_5d=True
    )
    val_dataset = SurgicalVideoDataset(
        csv_files['val.csv'], CONFIG['base_path'], return_5d=True
    )
    test_dataset = SurgicalVideoDataset(
        csv_files['test.csv'], CONFIG['base_path'], return_5d=True
    )

    # Data loaders
    train_loader = DataLoader(
        train_dataset, batch_size=CONFIG['batch_size'], sampler=train_sampler,
        shuffle=False, num_workers=CONFIG['num_workers'], pin_memory=True, prefetch_factor=2
    )
    val_loader = DataLoader(
        val_dataset, batch_size=CONFIG['batch_size'], shuffle=False,
        num_workers=CONFIG['num_workers'], pin_memory=True, prefetch_factor=2
    )
    test_loader = DataLoader(
        test_dataset, batch_size=CONFIG['batch_size'], shuffle=False,
        num_workers=CONFIG['num_workers'], pin_memory=True, prefetch_factor=2
    )

    # Initialize model
    print("\nInitializing ResNet18-TSM (Two-Stream, On-the-Fly Flow)...")
    model = ResNetTSM_OF(
        num_classes=CONFIG['num_classes'],
        num_segments=16,
        depth=CONFIG['model_depth'],
        pretrained=True,
        use_flow=CONFIG['use_flow']
    ).to(CONFIG['device'])

    # **LOAD EXISTING CHECKPOINT FOR RGB STREAM**
    print(f"\nLoading existing checkpoint: {CONFIG['checkpoint_load']}")
    try:
        checkpoint = torch.load(CONFIG['checkpoint_load'], map_location=CONFIG['device'])

        # Load RGB stream weights
        rgb_keys = [k for k in checkpoint.keys() if not k.startswith('flow_') and not k.startswith('fusion')]
        rgb_checkpoint = {k: checkpoint[k] for k in rgb_keys if k in model.state_dict()}
        model.load_state_dict(rgb_checkpoint, strict=False)
        print("✓ Loaded RGB stream from checkpoint")
    except Exception as e:
        print(f"Warning: Could not load checkpoint: {e}")
        print("Training from scratch instead")

    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Total parameters: {total_params:,}")
    print(f"Trainable parameters: {trainable_params:,}")

    # Optimizer
    optimizer = optim.SGD(
        model.parameters(), lr=CONFIG['learning_rate'], momentum=0.9,
        weight_decay=CONFIG['weight_decay']
    )
    criterion = nn.CrossEntropyLoss()
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['num_epochs'])
    scaler = torch.cuda.amp.GradScaler()

    # Training loop
    best_val_acc = 0.0
    train_losses, val_losses = [], []
    train_accs, val_accs = [], []
    train_times, val_inference_times = [], []

    print("\n" + "="*80)
    print("Starting Training - ResNet18-TSM (Two-Stream, On-the-Fly Optical Flow)")
    print("="*80)

    training_start = time.time()

    for epoch in range(CONFIG['num_epochs']):
        print(f"\nEpoch {epoch + 1}/{CONFIG['num_epochs']}")

        train_loss, train_acc, train_time = train_epoch(
            model, train_loader, optimizer, criterion, CONFIG['device'], scaler
        )
        val_loss, val_acc, _, _, val_inference_time = validate(
            model, val_loader, criterion, CONFIG['device']
        )

        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_accs.append(train_acc)
        val_accs.append(val_acc)
        train_times.append(train_time)
        val_inference_times.append(val_inference_time)

        print(f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Time: {train_time:.2f}s")
        print(f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Inference: {val_inference_time*1000:.2f}ms")

        scheduler.step()

        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), CONFIG['checkpoint_dir'] / 'best_model_twostream.pth')
            print(f"✓ Saved best two-stream model")

    total_training_time = time.time() - training_start

    # Evaluate on test set
    print("\n" + "="*80)
    print("Evaluating on Test Set")
    print("="*80)

    model.load_state_dict(torch.load(CONFIG['checkpoint_dir'] / 'best_model_twostream.pth'))
    test_loss, test_acc, test_preds, test_labels, test_inference_time = validate(
        model, test_loader, criterion, CONFIG['device']
    )

    print(f"\nTest Accuracy: {test_acc:.4f}")
    print(f"Test Loss: {test_loss:.4f}")
    print(f"Inference Time per Sample: {test_inference_time*1000:.2f}ms")
    print(f"16-frame latency: {test_inference_time*16*1000:.2f}ms")

    # Per-class metrics
    precision, recall, f1, _ = precision_recall_fscore_support(
        test_labels, test_preds, average='weighted'
    )
    precision_per_class, recall_per_class, f1_per_class, support = precision_recall_fscore_support(
        test_labels, test_preds, average=None
    )

    print(f"\nWeighted Metrics:")
    print(f"Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f}")

    # Per-class metrics table
    print("\n" + "="*80)
    print("Per-Class Metrics:")
    print("="*80)

    class_names = [test_dataset.idx2label[i] for i in range(len(test_dataset.idx2label))]

    metrics_table = pd.DataFrame({
        'Class': class_names,
        'Precision': precision_per_class,
        'Recall': recall_per_class,
        'F1-Score': f1_per_class,
        'Support': support.astype(int)
    })

    print(metrics_table.to_string(index=False))

    # Classification report
    print("\n" + "="*80)
    print("Classification Report:")
    print("="*80)
    class_report = classification_report(test_labels, test_preds, target_names=class_names)
    print(class_report)

    # Summary
    print("\n" + "="*80)
    print("Training Summary")
    print("="*80)
    print(f"Total Time: {total_training_time/60:.2f} minutes")
    print(f"Best Val Acc: {best_val_acc:.4f}")
    print(f"Test Acc: {test_acc:.4f}")

    # Save metrics
    metrics = {
        'model': 'ResNet18-TSM-TwoStream-OnTheFlyFlow',
        'use_flow': True,
        'flow_computed': 'On-the-fly (OpenCV Farneback)',
        'test_acc': float(test_acc),
        'test_precision_weighted': float(precision),
        'test_recall_weighted': float(recall),
        'test_f1_weighted': float(f1),
        'per_class_metrics': {
            'precision': [float(p) for p in precision_per_class],
            'recall': [float(r) for r in recall_per_class],
            'f1': [float(f) for f in f1_per_class],
            'support': [int(s) for s in support],
            'class_names': class_names
        },
        'total_time_minutes': total_training_time/60,
        'inference_time_ms': test_inference_time*1000,
        'latency_16frames_ms': test_inference_time*16*1000,
    }

    with open(CONFIG['checkpoint_dir'] / 'metrics.json', 'w') as f:
        json.dump(metrics, f, indent=4)

    # Save classification report to file
    with open(CONFIG['checkpoint_dir'] / 'classification_report.txt', 'w') as f:
        f.write("Per-Class Metrics:\n")
        f.write(metrics_table.to_string(index=False))
        f.write("\n\n")
        f.write("Classification Report:\n")
        f.write(class_report)

    print("✓ Saved classification report and metrics")

    # Confusion matrix
    cm = confusion_matrix(test_labels, test_preds)

    plt.figure(figsize=(12, 10))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names,
                cbar_kws={'label': 'Count'})
    plt.title(f'Confusion Matrix - Test Set\nOverall Accuracy: {test_acc:.4f}')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    plt.savefig(CONFIG['checkpoint_dir'] / 'confusion_matrix.png', dpi=150)
    plt.show()
    print("✓ Saved confusion matrix")

    # Plot per-class metrics
    fig, axes = plt.subplots(1, 3, figsize=(16, 5))

    x_pos = np.arange(len(class_names))

    axes[0].bar(x_pos, precision_per_class, color='skyblue', edgecolor='navy')
    axes[0].set_ylabel('Precision')
    axes[0].set_title('Precision per Class')
    axes[0].set_xticks(x_pos)
    axes[0].set_xticklabels(class_names, rotation=45, ha='right')
    axes[0].axhline(y=precision, color='r', linestyle='--', label='Weighted Mean')
    axes[0].legend()
    axes[0].grid(axis='y', alpha=0.3)

    axes[1].bar(x_pos, recall_per_class, color='lightgreen', edgecolor='darkgreen')
    axes[1].set_ylabel('Recall')
    axes[1].set_title('Recall per Class')
    axes[1].set_xticks(x_pos)
    axes[1].set_xticklabels(class_names, rotation=45, ha='right')
    axes[1].axhline(y=recall, color='r', linestyle='--', label='Weighted Mean')
    axes[1].legend()
    axes[1].grid(axis='y', alpha=0.3)

    axes[2].bar(x_pos, f1_per_class, color='salmon', edgecolor='darkred')
    axes[2].set_ylabel('F1-Score')
    axes[2].set_title('F1-Score per Class')
    axes[2].set_xticks(x_pos)
    axes[2].set_xticklabels(class_names, rotation=45, ha='right')
    axes[2].axhline(y=f1, color='r', linestyle='--', label='Weighted Mean')
    axes[2].legend()
    axes[2].grid(axis='y', alpha=0.3)

    plt.tight_layout()
    plt.savefig(CONFIG['checkpoint_dir'] / 'per_class_metrics.png', dpi=150)
    plt.show()
    print("✓ Saved per-class metrics plot")

    # Training curves
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))

    axes[0, 0].plot(train_losses, label='Train', marker='o', markersize=4)
    axes[0, 0].plot(val_losses, label='Val', marker='s', markersize=4)
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].set_title('Training Curves - Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)

    axes[0, 1].plot(train_accs, label='Train', marker='o', markersize=4)
    axes[0, 1].plot(val_accs, label='Val', marker='s', markersize=4)
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Accuracy')
    axes[0, 1].set_title('Training Curves - Accuracy')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)

    axes[1, 0].plot(train_times, label='Train Time', marker='o', color='green', markersize=4)
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('Time (seconds)')
    axes[1, 0].set_title('Training Time per Epoch')
    axes[1, 0].grid(True, alpha=0.3)

    axes[1, 1].plot([t*1000 for t in val_inference_times], label='Inference Time', marker='s', color='orange', markersize=4)
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('Time (ms per sample)')
    axes[1, 1].set_title('Average Validation Inference Time per Sample')
    axes[1, 1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(CONFIG['checkpoint_dir'] / 'training_curves.png', dpi=150)
    plt.show()
    print("✓ Saved training curves")

    print("\n✓ Training Complete!")

from pathlib import Path

# Add the missing CONFIG
CONFIG = {
    'base_path': Path('/home/zhadiger/Desktop/data_preprocessing/dataset/clips'),
    'num_classes': 7,
    'batch_size': 4,
    'num_epochs': 15,
    'learning_rate': 1e-3,
    'weight_decay': 0,
    'num_workers': 2,
    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),
    'seed': 42,
    'checkpoint_dir': Path(''),
    'checkpoint_load': Path('/home/zhadiger/Desktop/data_preprocessing/best_model_twostream.pth'),
    'use_flow': True,
    'tau_sampling': 0.5,  # ← THIS WAS MISSING
    'model_depth': 18,
}

# Now run your training
csv_files = {
    'train.csv': Path('/home/zhadiger/Desktop/data_preprocessing/dataset/train.csv'),
    'val.csv': Path('/home/zhadiger/Desktop/data_preprocessing/dataset/val.csv'),
    'test.csv': Path('/home/zhadiger/Desktop/data_preprocessing/test.csv'),
}

main(csv_files)